{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viny2030/tensorflow/blob/main/1bert_preprocessing_guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ],
      "metadata": {
        "id": "Tce3stUlHN0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Preprocessing with TF Text"
      ],
      "metadata": {
        "id": "qFdPvlXBOdUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/guide/bert_preprocessing_guide\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/bert_preprocessing_guide.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/guide/bert_preprocessing_guide.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/guide/bert_preprocessing_guide.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "MfBg1C5NB3X0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Text preprocessing is the end-to-end transformation of raw text into a model’s integer inputs. NLP models are often accompanied by several hundreds (if not thousands) of lines of Python code for preprocessing text. Text preprocessing is often a challenge for models because:\n",
        "\n",
        "* **Training-serving skew.** It becomes increasingly difficult to ensure that the preprocessing logic of the model's inputs are consistent at all stages of model development (e.g. pretraining, fine-tuning, evaluation, inference).\n",
        "Using different hyperparameters, tokenization, string preprocessing algorithms or simply packaging model inputs inconsistently at different stages could yield hard-to-debug and disastrous effects to the model.\n",
        "\n",
        "* **Efficiency and flexibility.** While preprocessing can be done offline (e.g. by writing out processed outputs to files on disk and then reconsuming said preprocessed data in the input pipeline), this method incurs an additional file read and write cost. Preprocessing offline is also inconvenient if there are preprocessing decisions that need to happen dynamically. Experimenting with a different option would require regenerating the dataset again.\n",
        "\n",
        "* **Complex model interface.** Text models are much more understandable when their inputs are pure text. It's hard to understand a model when its inputs require an extra, indirect encoding step. Reducing the preprocessing complexity is especially appreciated for model debugging, serving, and evaluation.\n",
        "\n",
        "Additionally, simpler model interfaces also make it more convenient to try the model (e.g. inference or training) on different, unexplored datasets.\n"
      ],
      "metadata": {
        "id": "xHxb-dlhMIzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text preprocessing with TF.Text\n",
        "\n",
        "Using TF.Text's text preprocessing APIs, we can construct a preprocessing\n",
        "function that can transform a user's text dataset into the model's\n",
        "integer inputs. Users can package preprocessing directly as part of their model to alleviate the above mentioned problems.\n",
        "\n",
        "This tutorial will show how to use TF.Text preprocessing ops to transform text data into inputs for the BERT model and inputs for language masking pretraining task described in \"Masked LM and Masking Procedure\" of [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf). The process involves tokenizing text into subword units, combining sentences, trimming content to a fixed size and extracting labels for the masked language modeling task."
      ],
      "metadata": {
        "id": "Y6DTHtXbxPgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocesamiento de texto con TF.Text**\n",
        "Usando las API de preprocesamiento de texto de TF.Text, podemos construir una función de preprocesamiento que puede transformar el conjunto de datos de texto de un usuario en las entradas de números enteros del modelo. Los usuarios pueden empaquetar el preprocesamiento directamente como parte de su modelo para aliviar los problemas mencionados anteriormente.\n",
        "\n",
        "Este tutorial mostrará cómo usar las operaciones de preprocesamiento de TF.Text para transformar datos de texto en entradas para el modelo BERT y entradas para la tarea de preentrenamiento de enmascaramiento de lenguaje descrita en \"LM enmascarado y procedimiento de enmascaramiento\" de BERT: preentrenamiento de transformadores bidireccionales profundos para la comprensión del lenguaje. El proceso implica convertir el texto en unidades de subpalabras, combinar oraciones, recortar el contenido a un tamaño fijo y extraer etiquetas para la tarea de modelado de lenguaje enmascarado.\n"
      ],
      "metadata": {
        "id": "Y0x8-z2MTO4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "MUXex9ctTuDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import the packages and libraries we need first."
      ],
      "metadata": {
        "id": "pmIjNKsfeTpm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install -q -U \"tensorflow-text==2.11.*\""
      ],
      "outputs": [],
      "metadata": {
        "id": "gTWQ5swI7FRJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "import functools\n",
        "print(\"TensorFlow version: \", tf.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version:  2.11.1\n"
          ]
        }
      ],
      "metadata": {
        "id": "IqR2PQG4ZaZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb9b245d-6f82-42e5-c363-ef8a39729dea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data contains two text features and we can create a example `tf.data.Dataset`. Our goal is to create a function that we can supply `Dataset.map()` with to be used in training."
      ],
      "metadata": {
        "id": "-brDHSrRaMii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuestros datos contienen dos características de texto y podemos crear un ejemplo tf.data.Dataset. Nuestro objetivo es crear una función que podamos suministrar a Dataset.map() para que se use en el entrenamiento."
      ],
      "metadata": {
        "id": "dzdOu9ewT5wf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "examples = {\n",
        "    \"text_a\": [\n",
        "      \"Sponge bob Squarepants is an Avenger\",\n",
        "      \"Marvel Avengers\"\n",
        "    ],\n",
        "    \"text_b\": [\n",
        "     \"Barack Obama is the President.\",\n",
        "     \"President is the highest office\"\n",
        "  ],\n",
        "}\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
        "next(iter(dataset))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text_a': <tf.Tensor: shape=(), dtype=string, numpy=b'Sponge bob Squarepants is an Avenger'>,\n",
              " 'text_b': <tf.Tensor: shape=(), dtype=string, numpy=b'Barack Obama is the President.'>}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "metadata": {
        "id": "DQyj7OQ9yk7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd85603-b961-4944-8bb5-5bad9dd8abb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples1 = {\n",
        "    \"text_a\": [\n",
        "      \"Jane bought me these books\",\n",
        "       \"Jane bought a book for me\",\n",
        "       \"She dropped a line to him\",\n",
        "       \"She sleeps\",\n",
        "        \"I sleep a lot\",\n",
        "      \"I was born in Madrid\"\n",
        "    ],\n",
        "    \"text_b\": [\n",
        "\"the cat was chased by the dog\",\n",
        "\"I was born in Madrid during 1995\",\n",
        "\"Out of all this,something good will come\",\n",
        "\"Susan left after the rehearsal. She did it well\",\n",
        "\"She sleeps during the morning\",\n",
        "\"but she sleeps\"\n",
        "  ],\n",
        "}\n",
        "dataset1 = tf.data.Dataset.from_tensor_slices(examples1)\n",
        "next(iter(dataset1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izgQl7anWxmR",
        "outputId": "8dbc8ec2-3a2f-46a3-da50-b908c8e03f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text_a': <tf.Tensor: shape=(), dtype=string, numpy=b'Jane bought me these books'>,\n",
              " 'text_b': <tf.Tensor: shape=(), dtype=string, numpy=b'the cat was chased by the dog'>}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing\n",
        "\n",
        "Our first step is to run any string preprocessing and tokenize our dataset. This can be done using the [`text.BertTokenizer`](https://tensorflow.org/text/api_docs/python/text/BertTokenizer), which is a [`text.Splitter`](https://tensorflow.org/text/api_docs/python/text/Splitter) that can tokenize sentences into subwords or wordpieces for the [BERT model](https://github.com/google-research/bert) given a vocabulary generated from the [Wordpiece algorithm](https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm). You can learn more about other subword tokenizers available in TF.Text from [here](https://www.tensorflow.org/text/guide/subwords_tokenizer).\n",
        "\n",
        "\n",
        "The vocabulary can be from a previously generated BERT checkpoint, or you can generate one yourself on your own data. For the purposes of this example, let's create a toy vocabulary:"
      ],
      "metadata": {
        "id": "1laUIs3g5Qsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenización\n",
        "Nuestro primer paso es ejecutar cualquier preprocesamiento de cadenas y tokenizar nuestro conjunto de datos. Esto se puede hacer usando text.BertTokenizer, que es un text.Splitter que puede tokenizar oraciones en subpalabras o fragmentos de palabras para el modelo BERT dado un vocabulario generado a partir del algoritmo Wordpiece. Puede obtener más información sobre otros tokenizadores de subpalabras disponibles en TF.Text aquí.\n",
        "\n",
        "El vocabulario puede provenir de un punto de control BERT generado previamente, o puede generar uno usted mismo con sus propios datos. Para los fines de este ejemplo, creemos un vocabulario de juguete:"
      ],
      "metadata": {
        "id": "FtsBYqzTUCgw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "_VOCAB = [\n",
        "    # Special tokens\n",
        "    b\"[UNK]\", b\"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\n",
        "    # Suffixes\n",
        "    b\"##ack\", b\"##ama\", b\"##ger\", b\"##gers\", b\"##onge\", b\"##pants\",  b\"##uare\",\n",
        "    b\"##vel\", b\"##ven\", b\"an\", b\"A\", b\"Bar\", b\"Hates\", b\"Mar\", b\"Ob\",\n",
        "    b\"Patrick\", b\"President\", b\"Sp\", b\"Sq\", b\"bob\", b\"box\", b\"has\", b\"highest\",\n",
        "    b\"is\", b\"office\", b\"the\",\n",
        "]\n",
        "\n",
        "_START_TOKEN = _VOCAB.index(b\"[CLS]\")\n",
        "_END_TOKEN = _VOCAB.index(b\"[SEP]\")\n",
        "_MASK_TOKEN = _VOCAB.index(b\"[MASK]\")\n",
        "_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
        "_UNK_TOKEN = _VOCAB.index(b\"[UNK]\")\n",
        "_MAX_SEQ_LEN = 8\n",
        "_MAX_PREDICTIONS_PER_BATCH = 5\n",
        "\n",
        "_VOCAB_SIZE = len(_VOCAB)\n",
        "\n",
        "lookup_table = tf.lookup.StaticVocabularyTable(\n",
        "    tf.lookup.KeyValueTensorInitializer(\n",
        "      keys=_VOCAB,\n",
        "      key_dtype=tf.string,\n",
        "      values=tf.range(\n",
        "          tf.size(_VOCAB, out_type=tf.int64), dtype=tf.int64),\n",
        "          value_dtype=tf.int64\n",
        "        ),\n",
        "      num_oov_buckets=1\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "ChpIFy515S1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's construct a [`text.BertTokenizer`](https://tensorflow.org/text/api_docs/python/text/BertTokenizer) using the above vocabulary and tokenize the text inputs into a [`RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor).`."
      ],
      "metadata": {
        "id": "7t2tgbSn6nvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construyamos un text.BertTokenizer usando el vocabulario anterior y tokenicemos las entradas de texto en un RaggedTensor.`."
      ],
      "metadata": {
        "id": "l2eZsh9aUJzF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bert_tokenizer = text.BertTokenizer(lookup_table, token_out_type=tf.string)\n",
        "bert_tokenizer.tokenize(examples[\"text_a\"])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[[b'Sp', b'##onge'], [b'bob'], [b'Sq', b'##uare', b'##pants'], [b'is'],\n",
              "  [b'an'], [b'A', b'##ven', b'##ger']]                                  ,\n",
              " [[b'Mar', b'##vel'], [b'A', b'##ven', b'##gers']]]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "metadata": {
        "id": "564UPrFB5Zm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff676be7-ee6f-4799-9ecd-53d903146902"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer1 = text.BertTokenizer(lookup_table, token_out_type=tf.string)\n",
        "bert_tokenizer1.tokenize(examples1[\"text_a\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIIjBf82YGXF",
        "outputId": "6b81da7a-8a54-474e-dec9-4ae6290d705e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[[b'[UNK]'],\n",
              "  [b'[UNK]'],\n",
              "  [b'[UNK]'],\n",
              "  [b'[UNK]'],\n",
              "  [b'[UNK]']], [[b'[UNK]'],\n",
              "                [b'[UNK]'],\n",
              "                [b'[UNK]'],\n",
              "                [b'[UNK]'],\n",
              "                [b'[UNK]'],\n",
              "                [b'[UNK]']], [[b'[UNK]'],\n",
              "                              [b'[UNK]'],\n",
              "                              [b'[UNK]'],\n",
              "                              [b'[UNK]'],\n",
              "                              [b'[UNK]'],\n",
              "                              [b'[UNK]']], [[b'[UNK]'],\n",
              "                                            [b'[UNK]']], [[b'[UNK]'],\n",
              "                                                          [b'[UNK]'],\n",
              "                                                          [b'[UNK]'],\n",
              "                                                          [b'[UNK]']],\n",
              " [[b'[UNK]'],\n",
              "  [b'[UNK]'],\n",
              "  [b'[UNK]'],\n",
              "  [b'[UNK]'],\n",
              "  [b'[UNK]']]]>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bert_tokenizer1.tokenize(examples1[\"text_b\"])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[[b'the'],\n",
              "  [b'[UNK]'],\n",
              "  [b'[UNK]'],\n",
              "  [b'[UNK]'],\n",
              "  [b'[UNK]'],\n",
              "  [b'the'],\n",
              "  [b'[UNK]']], [[b'[UNK]'],\n",
              "                [b'[UNK]'],\n",
              "                [b'[UNK]'],\n",
              "                [b'[UNK]'],\n",
              "                [b'[UNK]'],\n",
              "                [b'[UNK]'],\n",
              "                [b'[UNK]']], [[b'[UNK]'],\n",
              "                              [b'[UNK]'],\n",
              "                              [b'[UNK]'],\n",
              "                              [b'[UNK]'],\n",
              "                              [b'[UNK]'],\n",
              "                              [b'[UNK]'],\n",
              "                              [b'[UNK]'],\n",
              "                              [b'[UNK]'],\n",
              "                              [b'[UNK]']], [[b'[UNK]'],\n",
              "                                            [b'[UNK]'],\n",
              "                                            [b'[UNK]'],\n",
              "                                            [b'the'],\n",
              "                                            [b'[UNK]'],\n",
              "                                            [b'[UNK]'],\n",
              "                                            [b'[UNK]'],\n",
              "                                            [b'[UNK]'],\n",
              "                                            [b'[UNK]'],\n",
              "                                            [b'[UNK]']], [[b'[UNK]'],\n",
              "                                                          [b'[UNK]'],\n",
              "                                                          [b'[UNK]'],\n",
              "                                                          [b'the'],\n",
              "                                                          [b'[UNK]']],\n",
              " [[b'[UNK]'],\n",
              "  [b'[UNK]'],\n",
              "  [b'[UNK]']]]>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "metadata": {
        "id": "AiTs3_FHHBlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecbf251b-3511-4391-95df-4f5a3255887a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text output from [`text.BertTokenizer`](https://tensorflow.org/text/api_docs/python/text/BertTokenizer) allows us see how the text is being tokenized, but the model requires integer IDs. We can set the `token_out_type` param to `tf.int64` to obtain integer IDs (which are the indices into the vocabulary)."
      ],
      "metadata": {
        "id": "cK6DHjio65MV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La salida de texto de text.BertTokenizer nos permite ver cómo se está tokenizando el texto, pero el modelo requiere identificadores enteros. Podemos establecer el parámetro token_out_type en tf.int64 para obtener identificadores enteros (que son los índices del vocabulario)."
      ],
      "metadata": {
        "id": "GCx_nAxkUPr9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bert_tokenizer1 = text.BertTokenizer(lookup_table, token_out_type=tf.int64)\n",
        "segment_a1 = bert_tokenizer1.tokenize(examples[\"text_a\"])\n",
        "segment_a1"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[[22, 9], [24], [23, 11, 10], [28], [14], [15, 13, 7]],\n",
              " [[18, 12], [15, 13, 8]]]>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "metadata": {
        "id": "odeosiPz58Qu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04744558-b43d-4eb0-9088-34d5864f8202"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer1 = text.BertTokenizer(lookup_table, token_out_type=tf.int64)\n",
        "segment_a1 = bert_tokenizer1.tokenize(examples[\"text_a\"])\n",
        "segment_a1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNk1MouGYRoR",
        "outputId": "9a006f69-435e-4ef4-dc10-00080e234e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[[22, 9], [24], [23, 11, 10], [28], [14], [15, 13, 7]],\n",
              " [[18, 12], [15, 13, 8]]]>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "segment_b1 = bert_tokenizer1.tokenize(examples[\"text_b\"])\n",
        "segment_b1"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[[16, 5], [19, 6], [28], [30], [21], [0]], [[21], [28], [30], [27], [29]]]>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "metadata": {
        "id": "v4IP2P4EHQpa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb6913cb-fd3e-4085-bf89-2f43bfef3064"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`text.BertTokenizer`](https://tensorflow.org/text/api_docs/python/text/BertTokenizer) returns a `RaggedTensor` with shape `[batch, num_tokens, num_wordpieces]`. Because we don't need the extra `num_tokens` dimensions for our current use case,  we can merge the last two dimensions to obtain a `RaggedTensor` with shape `[batch, num_wordpieces]`:"
      ],
      "metadata": {
        "id": "TU3GJ0jx94fx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "text.BertTokenizer devuelve un RaggedTensor con forma [batch, num_tokens, num_wordpieces]. Como no necesitamos las dimensiones num_tokens adicionales para nuestro caso de uso actual, podemos fusionar las dos últimas dimensiones para obtener un RaggedTensor con forma [batch, num_wordpieces]:"
      ],
      "metadata": {
        "id": "hl6WHfJAUVPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "segment_a1 = segment_a1.merge_dims(-2, -1)\n",
        "segment_a1"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[22, 9, 24, 23, 11, 10, 28, 14, 15, 13, 7], [18, 12, 15, 13, 8]]>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "metadata": {
        "id": "Fb5vt5dA-Rwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfcece8d-74a3-4370-c767-96e20dab2be9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "segment_b1 = segment_b1.merge_dims(-2, -1)\n",
        "segment_b1"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[16, 5, 19, 6, 28, 30, 21, 0], [21, 28, 30, 27, 29]]>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "metadata": {
        "id": "NyEW0sjhHoPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c0d539-38dd-4a5d-904b-e27a576a10ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Content Trimming\n",
        "\n",
        "The main input to BERT is a concatenation of two sentences. However, BERT requires inputs to be in a fixed-size and shape and we may have content which exceed our budget.\n",
        "\n",
        "We can tackle this by using a [`text.Trimmer`](https://tensorflow.org/text/api_docs/python/text/Trimmer) to trim our content down to a predetermined size (once concatenated along the last axis). There are different `text.Trimmer` types which select content to preserve using different algorithms. [`text.RoundRobinTrimmer`](https://tensorflow.org/text/api_docs/python/text/RoundRobinTrimmer) for example will allocate quota equally for each segment but may trim the ends of sentences. [`text.WaterfallTrimmer`](https://tensorflow.org/text/api_docs/python/text/WaterfallTrimmer) will trim starting from the end of the last sentence.\n",
        "\n",
        "For our example, we will use `RoundRobinTrimmer` which selects items from each segment in a left-to-right manner.\n"
      ],
      "metadata": {
        "id": "R9YicLN5UFkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recorte de contenido\n",
        "La entrada principal de BERT es una concatenación de dos oraciones. Sin embargo, BERT requiere que las entradas tengan un tamaño y una forma fijos y es posible que tengamos contenido que exceda nuestro presupuesto.\n",
        "\n",
        "Podemos solucionar esto utilizando un text.Trimmer para recortar nuestro contenido a un tamaño predeterminado (una vez concatenado a lo largo del último eje). Hay diferentes tipos de text.Trimmer que seleccionan el contenido que se conservará utilizando diferentes algoritmos. text.RoundRobinTrimmer, por ejemplo, asignará una cuota equitativa para cada segmento, pero puede recortar los finales de las oraciones. text.WaterfallTrimmer recortará comenzando desde el final de la última oración.\n",
        "\n",
        "Para nuestro ejemplo, utilizaremos RoundRobinTrimmer, que selecciona elementos de cada segmento de izquierda a derecha."
      ],
      "metadata": {
        "id": "csn5GsLXUwvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "trimmer = text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN)\n",
        "trimmed = trimmer.trim([segment_a1, segment_b1])\n",
        "trimmed"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.RaggedTensor [[22, 9, 24, 23],\n",
              "  [18, 12, 15, 13]]>,\n",
              " <tf.RaggedTensor [[16, 5, 19, 6],\n",
              "  [21, 28, 30, 27]]>]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "metadata": {
        "id": "aLV-1uDgwFnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae356a92-3ada-4a1d-b5b1-80bd45ae2e8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`trimmed` now contains the segments where the number of elements across a batch is 8 elements (when concatenated along axis=-1)."
      ],
      "metadata": {
        "id": "zPj7jM9oQ-P3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "recortado ahora contiene los segmentos donde el número de elementos en un lote es de 8 elementos (cuando se concatenan a lo largo del eje = -1)."
      ],
      "metadata": {
        "id": "X_DehKGRU229"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining segments\n",
        "\n",
        "Now that we have segments trimmed, we can combine them together to get a single `RaggedTensor`. BERT uses special tokens to indicate the beginning (`[CLS]`) and end of a segment (`[SEP]`). We also need a `RaggedTensor` indicating which items in the combined `Tensor` belong to which segment. We can use [`text.combine_segments()`](https://tensorflow.org/text/api_docs/python/text/combine_segments) to get both of these `Tensor` with special tokens inserted."
      ],
      "metadata": {
        "id": "3J2AWfmAUio8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combinación de segmentos\n",
        "Ahora que hemos recortado los segmentos, podemos combinarlos para obtener un único tensor irregular. BERT utiliza tokens especiales para indicar el comienzo ([CLS]) y el final de un segmento ([SEP]). También necesitamos un tensor irregular que indique qué elementos del tensor combinado pertenecen a qué segmento. Podemos utilizar text.combine_segments() para obtener ambos tensores con tokens especiales insertados."
      ],
      "metadata": {
        "id": "cDZ8GlPHU7ks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "segments_combined, segments_ids = text.combine_segments(\n",
        "  trimmed,\n",
        "  start_of_sequence_id=_START_TOKEN, end_of_segment_id=_END_TOKEN)\n",
        "segments_combined, segments_ids"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.RaggedTensor [[3, 22, 9, 24, 23, 4, 16, 5, 19, 6, 4],\n",
              "  [3, 18, 12, 15, 13, 4, 21, 28, 30, 27, 4]]>,\n",
              " <tf.RaggedTensor [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "metadata": {
        "id": "L-5nMh5pk8x1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3239a089-b200-4798-f504-2324183f5281"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masked Language Model Task\n",
        "\n",
        "Now that we have our basic inputs, we can begin to extract the inputs needed for the \"Masked LM and Masking Procedure\" task described in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
        "\n",
        "The masked language model task has two sub-problems for us to think about: (1) what items to select for masking and (2) what values are they assigned?\n"
      ],
      "metadata": {
        "id": "hSKla2OxUOWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tarea de modelo de lenguaje enmascarado**\n",
        "Ahora que tenemos nuestras entradas básicas, podemos comenzar a extraer las entradas necesarias para la tarea \"LM enmascarado y procedimiento de enmascaramiento\" descrita en BERT: preentrenamiento de transformadores bidireccionales profundos para la comprensión del lenguaje\n",
        "\n",
        "La tarea de modelo de lenguaje enmascarado tiene dos subproblemas en los que debemos pensar: (1) qué elementos seleccionar para enmascarar y (2) ¿qué valores se les asignan?\n"
      ],
      "metadata": {
        "id": "vF0zAZ1NVByd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Item Selection\n",
        "Because we will choose to select items randomly for masking, we will use a [`text.RandomItemSelector`](https://tensorflow.org/text/api_docs/python/text/RandomItemSelector). `RandomItemSelector` randomly selects items in a batch subject to restrictions given (`max_selections_per_batch`, `selection_rate` and `unselectable_ids`) and returns a boolean mask indicating which items were selected."
      ],
      "metadata": {
        "id": "mkx4w9-3DT0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selección de elementos\n",
        "Como elegiremos seleccionar elementos al azar para enmascararlos, utilizaremos un text.RandomItemSelector. RandomItemSelector selecciona elementos al azar en un lote sujeto a restricciones dadas (max_selections_per_batch, selection_rate y unselectable_ids) y devuelve una máscara booleana que indica qué elementos fueron seleccionados"
      ],
      "metadata": {
        "id": "BdBqOfgrVK0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "random_selector = text.RandomItemSelector(\n",
        "    max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH,\n",
        "    selection_rate=0.2,\n",
        "    unselectable_ids=[_START_TOKEN, _END_TOKEN, _UNK_TOKEN]\n",
        ")\n",
        "selected = random_selector.get_selection_mask(\n",
        "    segments_combined, axis=1)\n",
        "selected"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[False, False, False, False, True, False, True, False, False, False,\n",
              "  False],\n",
              " [False, True, False, False, False, False, True, False, False, False,\n",
              "  False]]>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "metadata": {
        "id": "94BncqVkVJT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d226e7-2aa8-4d0e-ddec-c37b2415fab2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Choosing the Masked Value\n",
        "\n",
        "The methodology described the original BERT paper for choosing the value for masking is as follows:\n",
        "\n",
        "For `mask_token_rate` of the time, replace the item with the `[MASK]` token:\n",
        "\n",
        "    \"my dog is hairy\" -> \"my dog is [MASK]\"\n",
        "\n",
        "For `random_token_rate` of the time, replace the item with a random word:\n",
        "\n",
        "    \"my dog is hairy\" -> \"my dog is apple\"\n",
        "\n",
        "For `1 - mask_token_rate - random_token_rate` of the time, keep the item\n",
        "unchanged:\n",
        "\n",
        "    \"my dog is hairy\" -> \"my dog is hairy.\"\n",
        "\n",
        "[`text.MaskedValuesChooser`](https://tensorflow.org/text/api_docs/python/text/MaskValuesChooser) encapsulates this logic and can be used for our preprocessing function. Here's an example of what `MaskValuesChooser` returns given a `mask_token_rate` of 80% and default `random_token_rate`:\n"
      ],
      "metadata": {
        "id": "p4NAHL_GUi-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Elección del valor enmascarado\n",
        "La metodología descrita en el artículo original de BERT para elegir el valor de enmascaramiento es la siguiente:\n",
        "\n",
        "Para mask_token_rate del tiempo, reemplace el elemento con el token [MASK]:\n",
        "\n",
        "\"mi perro es peludo\" -> \"mi perro es [MASK]\"\n",
        "Para random_token_rate del tiempo, reemplace el elemento con una palabra aleatoria:\n",
        "\n",
        "\"mi perro es peludo\" -> \"mi perro es manzana\"\n",
        "Para 1 - mask_token_rate - random_token_rate del tiempo, mantenga el elemento sin cambios:\n",
        "\n",
        "\"mi perro es peludo\" -> \"mi perro es peludo\".\n",
        "text.MaskedValuesChooser encapsula esta lógica y se puede utilizar para nuestra función de preprocesamiento. Aquí hay un ejemplo de lo que MaskValuesChooser devuelve dado un mask_token_rate del 80% y un random_token_rate predeterminado:\n"
      ],
      "metadata": {
        "id": "zTEbKRBCVWgB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "mask_values_chooser = text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, 0.8)\n",
        "mask_values_chooser.get_mask_values(segments_combined)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[1, 1, 1, 1, 23, 1, 1, 1, 1, 1, 23],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "metadata": {
        "id": "Amk0Lqd5VJ4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6e20c98-a1b8-4b0c-b82d-9fef90b08da4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When supplied with a `RaggedTensor` input, `text.MaskValuesChooser` returns a `RaggedTensor` of the same shape with either `_MASK_VALUE` (0), a random ID, or the same unchanged id."
      ],
      "metadata": {
        "id": "UCp1CQcPC6IT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuando se suministra con una entrada RaggedTensor, text.MaskValuesChooser devuelve un RaggedTensor de la misma forma con _MASK_VALUE (0), una ID aleatoria o la misma ID sin cambios."
      ],
      "metadata": {
        "id": "IqnMO5FAVeL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generating Inputs for Masked Language Model Task\n",
        "\n",
        "Now that we have a `RandomItemSelector` to help us select items for masking and `text.MaskValuesChooser` to assign the values, we can use [`text.mask_language_model()`](https://tensorflow.org/text/api_docs/python/text/mask_language_model) to assemble all the inputs of this task for our BERT model.\n"
      ],
      "metadata": {
        "id": "EYpKg_sLUi1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generación de entradas para la tarea del modelo de lenguaje enmascarado\n",
        "Ahora que tenemos un RandomItemSelector para ayudarnos a seleccionar elementos para enmascarar y un text.MaskValuesChooser para asignar los valores, podemos usar text.mask_language_model() para ensamblar todas las entradas de esta tarea para nuestro modelo BERT."
      ],
      "metadata": {
        "id": "NaQD1Ez1VkMd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "masked_token_ids, masked_pos, masked_lm_ids = text.mask_language_model(\n",
        "  segments_combined,\n",
        "  item_selector=random_selector, mask_values_chooser=mask_values_chooser)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Q0fqQzXGUrkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's dive deeper and examine the outputs of `mask_language_model()`. The output of `masked_token_ids` is:"
      ],
      "metadata": {
        "id": "pJqcbOJ0AYBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Profundicemos más y examinemos los resultados de mask_language_model(). El resultado de masked_token_ids es:"
      ],
      "metadata": {
        "id": "ammT8RAQVowt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "masked_token_ids"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[3, 1, 1, 24, 23, 4, 16, 5, 19, 6, 4],\n",
              " [3, 1, 1, 15, 13, 4, 21, 28, 30, 27, 4]]>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "metadata": {
        "id": "PavYEhmN_tHa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16283db6-cd0f-4b33-f54d-9e2a0cc947ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that our input is encoded using a vocabulary. If we decode `masked_token_ids` using our vocabulary, we get:"
      ],
      "metadata": {
        "id": "Q0c2wkC9AnUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recuerde que nuestra entrada está codificada mediante un vocabulario. Si decodificamos masked_token_ids mediante nuestro vocabulario, obtenemos:"
      ],
      "metadata": {
        "id": "qSNnRxv2V3Mr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tf.gather(_VOCAB, masked_token_ids)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[CLS]', b'[MASK]', b'[MASK]', b'bob', b'Sq', b'[SEP]', b'Bar',\n",
              "  b'##ack', b'Ob', b'##ama', b'[SEP]'],\n",
              " [b'[CLS]', b'[MASK]', b'[MASK]', b'A', b'##ven', b'[SEP]', b'President',\n",
              "  b'is', b'the', b'highest', b'[SEP]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "metadata": {
        "id": "5axqrUOc_0h1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a2fc591-3e57-4765-8f64-71ef0afd4718"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that some wordpiece tokens have been replaced with either `[MASK]`, `[RANDOM]` or a different ID value. `masked_pos` output gives us the indices (in the respective batch) of the tokens that have been replaced."
      ],
      "metadata": {
        "id": "v8DCOtEAiz_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenga en cuenta que algunos tokens de piezas de palabras han sido reemplazados con [MASK], [RANDOM] o un valor de identificación diferente. La salida de masked_pos nos da los índices (en el lote respectivo) de los tokens que han sido reemplazados."
      ],
      "metadata": {
        "id": "-eIjvIabV9vg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "masked_pos"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[1, 2],\n",
              " [1, 2]]>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "metadata": {
        "id": "d-nc5m5Y_wP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "156a3876-c3b4-46ef-b360-28881aaa1550"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`masked_lm_ids` gives us the original value of the token."
      ],
      "metadata": {
        "id": "6fua7ANijN3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "masked_lm_ids nos da el valor original del token."
      ],
      "metadata": {
        "id": "fhVtXD4yWC8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "masked_lm_ids"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[22, 9],\n",
              " [18, 12]]>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "metadata": {
        "id": "azzxmO_f_xJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b08ed07-d842-41b9-f39f-20241abcec3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can again decode the IDs here to get human readable values."
      ],
      "metadata": {
        "id": "5bW0rdX9jYh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Podemos nuevamente decodificar los ID aquí para obtener valores legibles para humanos."
      ],
      "metadata": {
        "id": "rjbuSp1NWLS4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tf.gather(_VOCAB, masked_lm_ids)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'Sp', b'##onge'],\n",
              " [b'Mar', b'##vel']]>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "metadata": {
        "id": "F-RP-paUjUuP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7e7319-d366-4a9f-c02e-3767f1ed9734"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding Model Inputs\n",
        "\n",
        "Now that we have all the inputs for our model, the last step in our preprocessing is to package them into fixed 2-dimensional `Tensor`s with padding and also generate a mask `Tensor` indicating the values which are pad values. We can use [`text.pad_model_inputs()`](https://tensorflow.org/text/api_docs/python/text/pad_model_inputs) to help us with this task."
      ],
      "metadata": {
        "id": "3-P0PTiCUz2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entradas del modelo con relleno\n",
        "Ahora que tenemos todas las entradas para nuestro modelo, el último paso en nuestro preprocesamiento es empaquetarlas en tensores fijos bidimensionales con relleno y también generar un tensor de máscara que indique los valores que son valores de relleno. Podemos usar text.pad_model_inputs() para ayudarnos con esta tarea."
      ],
      "metadata": {
        "id": "dkqWiMYvWSXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Prepare and pad combined segment inputs\n",
        "input_word_ids, input_mask = text.pad_model_inputs(\n",
        "  masked_token_ids, max_seq_length=_MAX_SEQ_LEN)\n",
        "input_type_ids, _ = text.pad_model_inputs(\n",
        "  segments_ids, max_seq_length=_MAX_SEQ_LEN)\n",
        "\n",
        "# Prepare and pad masking task inputs\n",
        "masked_lm_positions, masked_lm_weights = text.pad_model_inputs(\n",
        "  masked_pos, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
        "masked_lm_ids, _ = text.pad_model_inputs(\n",
        "  masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
        "\n",
        "model_inputs = {\n",
        "    \"input_word_ids\": input_word_ids,\n",
        "    \"input_mask\": input_mask,\n",
        "    \"input_type_ids\": input_type_ids,\n",
        "    \"masked_lm_ids\": masked_lm_ids,\n",
        "    \"masked_lm_positions\": masked_lm_positions,\n",
        "    \"masked_lm_weights\": masked_lm_weights,\n",
        "}\n",
        "model_inputs"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_word_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
              " array([[ 3,  1,  1, 24, 23,  4, 16,  5],\n",
              "        [ 3,  1,  1, 15, 13,  4, 21, 28]])>,\n",
              " 'input_mask': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1]])>,\n",
              " 'input_type_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
              " array([[0, 0, 0, 0, 0, 0, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 1, 1]])>,\n",
              " 'masked_lm_ids': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
              " array([[22,  9,  0,  0,  0],\n",
              "        [18, 12,  0,  0,  0]])>,\n",
              " 'masked_lm_positions': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
              " array([[1, 2, 0, 0, 0],\n",
              "        [1, 2, 0, 0, 0]])>,\n",
              " 'masked_lm_weights': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
              " array([[1, 1, 0, 0, 0],\n",
              "        [1, 1, 0, 0, 0]])>}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "metadata": {
        "id": "FGE7XuXRwsYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fdc36bd-26f8-4f35-e771-5cad766e83b3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review"
      ],
      "metadata": {
        "id": "KIWy4nVyT6gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's review what we have so far and assemble our preprocessing function. Here's what we have:"
      ],
      "metadata": {
        "id": "TwCdO1Z5yjS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisión\n",
        "Repasemos lo que tenemos hasta ahora y ensamblemos nuestra función de preprocesamiento. Esto es lo que tenemos:"
      ],
      "metadata": {
        "id": "D8-893zxWYr9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def bert_pretrain_preprocess(vocab_table, features):\n",
        "  # Input is a string Tensor of documents, shape [batch, 1].\n",
        "  text_a = features[\"text_a\"]\n",
        "  text_b = features[\"text_b\"]\n",
        "\n",
        "  # Tokenize segments to shape [num_sentences, (num_words)] each.\n",
        "  tokenizer = text.BertTokenizer(\n",
        "      vocab_table,\n",
        "      token_out_type=tf.int64)\n",
        "  segments = [tokenizer.tokenize(text).merge_dims(\n",
        "      1, -1) for text in (text_a, text_b)]\n",
        "\n",
        "  # Truncate inputs to a maximum length.\n",
        "  trimmer = text.RoundRobinTrimmer(max_seq_length=6)\n",
        "  trimmed_segments = trimmer.trim(segments)\n",
        "\n",
        "  # Combine segments, get segment ids and add special tokens.\n",
        "  segments_combined, segment_ids = text.combine_segments(\n",
        "      trimmed_segments,\n",
        "      start_of_sequence_id=_START_TOKEN,\n",
        "      end_of_segment_id=_END_TOKEN)\n",
        "\n",
        "  # Apply dynamic masking task.\n",
        "  masked_input_ids, masked_lm_positions, masked_lm_ids = (\n",
        "      text.mask_language_model(\n",
        "        segments_combined,\n",
        "        random_selector,\n",
        "        mask_values_chooser,\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Prepare and pad combined segment inputs\n",
        "  input_word_ids, input_mask = text.pad_model_inputs(\n",
        "    masked_input_ids, max_seq_length=_MAX_SEQ_LEN)\n",
        "  input_type_ids, _ = text.pad_model_inputs(\n",
        "    segment_ids, max_seq_length=_MAX_SEQ_LEN)\n",
        "\n",
        "  # Prepare and pad masking task inputs\n",
        "  masked_lm_positions, masked_lm_weights = text.pad_model_inputs(\n",
        "    masked_lm_positions, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
        "  masked_lm_ids, _ = text.pad_model_inputs(\n",
        "    masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
        "\n",
        "  model_inputs = {\n",
        "      \"input_word_ids\": input_word_ids,\n",
        "      \"input_mask\": input_mask,\n",
        "      \"input_type_ids\": input_type_ids,\n",
        "      \"masked_lm_ids\": masked_lm_ids,\n",
        "      \"masked_lm_positions\": masked_lm_positions,\n",
        "      \"masked_lm_weights\": masked_lm_weights,\n",
        "  }\n",
        "  return model_inputs"
      ],
      "outputs": [],
      "metadata": {
        "id": "7jKtbVCYTsIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We previously constructed a `tf.data.Dataset` and we can now use our assembled preprocessing function `bert_pretrain_preprocess()` in `Dataset.map()`. This allows us to create an input pipeline for transforming our raw string data into integer inputs and feed directly into our model."
      ],
      "metadata": {
        "id": "bOAeo97VyfQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anteriormente construimos un tf.data.Dataset y ahora podemos usar nuestra función de preprocesamiento ensamblada bert_pretrain_preprocess() en Dataset.map(). Esto nos permite crear una secuencia de entrada para transformar nuestros datos de cadena sin procesar en entradas de números enteros y alimentarlos directamente a nuestro modelo."
      ],
      "metadata": {
        "id": "DeVfhua3WfXM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dataset = (\n",
        "    tf.data.Dataset.from_tensors(examples)\n",
        "    .map(functools.partial(bert_pretrain_preprocess, lookup_table))\n",
        ")\n",
        "\n",
        "next(iter(dataset))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_word_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
              " array([[ 3, 22,  1, 24,  4, 16,  5, 19],\n",
              "        [ 3, 18, 12,  1,  4, 21,  1, 30]])>,\n",
              " 'input_mask': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1]])>,\n",
              " 'input_type_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
              " array([[0, 0, 0, 0, 0, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 1, 1, 1]])>,\n",
              " 'masked_lm_ids': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
              " array([[ 9, 16,  0,  0,  0],\n",
              "        [15, 28,  0,  0,  0]])>,\n",
              " 'masked_lm_positions': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
              " array([[2, 5, 0, 0, 0],\n",
              "        [3, 6, 0, 0, 0]])>,\n",
              " 'masked_lm_weights': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
              " array([[1, 1, 0, 0, 0],\n",
              "        [1, 1, 0, 0, 0]])>}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "metadata": {
        "id": "xm4gTLEgjTa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "160dbba1-6e03-4a82-ee61-64f6904391ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Related Tutorials\n",
        "\n",
        "* [Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) - A tutorial on how to use a pretrained BERT model to classify text. This is a nice follow up now that you are familiar with how to preprocess the inputs used by the BERT model.\n",
        "\n",
        "* [Tokenizing with TF Text](https://www.tensorflow.org/text/guide/tokenizers) - Tutorial detailing the different types of tokenizers that exist in TF.Text.\n",
        "\n",
        "* [Handling Text with `RaggedTensor`](https://www.tensorflow.org/guide/ragged_tensor) - Detailed guide on how to create, use and manipulate `RaggedTensor`s.\n"
      ],
      "metadata": {
        "id": "FyiMxeEp0m2O"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}